#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Importation des biblioth√®ques
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
import warnings
warnings.filterwarnings('ignore')

# Configuration de l'affichage
plt.rcParams['figure.figsize'] = (10, 6)
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("‚úì Biblioth√®ques import√©es avec succ√®s !")


# In[ ]:


# Chargement des donn√©es Iris
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target
df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

# Afficher les premi√®res lignes
print("=== PREMI√àRES LIGNES DU DATAFRAME ===")
display(df.head())

print("\n=== INFORMATIONS SUR LES DONN√âES ===")
print(f"Nombre d'observations : {df.shape[0]}")
print(f"Nombre de variables : {df.shape[1]}")
print(f"\nVariables : {list(df.columns)}")


# In[ ]:


# Statistiques descriptives
print("=== STATISTIQUES DESCRIPTIVES ===")
display(df.describe())

# R√©partition des esp√®ces
print("\n=== R√âPARTITION DES ESP√àCES ===")
species_counts = df['species'].value_counts()
display(species_counts)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Diagramme en barres
axes[0].bar(species_counts.index, species_counts.values, color=['skyblue', 'lightgreen', 'salmon'])
axes[0].set_title('Distribution des esp√®ces')
axes[0].set_xlabel('Esp√®ce')
axes[0].set_ylabel('Nombre')

# Diagramme circulaire
axes[1].pie(species_counts.values, labels=species_counts.index, autopct='%1.1f%%',
            colors=['skyblue', 'lightgreen', 'salmon'], startangle=90)
axes[1].set_title('R√©partition en pourcentage')

plt.tight_layout()
plt.show()


# In[ ]:


# Histogrammes pour chaque variable
variables = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.flatten()

for i, var in enumerate(variables):
    axes[i].hist(df[var], bins=15, color='lightblue', edgecolor='black', alpha=0.7)
    axes[i].set_title(f'Distribution de {var}')
    axes[i].set_xlabel('Valeur (cm)')
    axes[i].set_ylabel('Fr√©quence')

    # Ajouter lignes statistiques
    mean_val = df[var].mean()
    median_val = df[var].median()
    axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Moyenne: {mean_val:.2f}')
    axes[i].axvline(median_val, color='green', linestyle='--', label=f'M√©diane: {median_val:.2f}')
    axes[i].legend()

plt.tight_layout()
plt.show()


# In[ ]:


# Nuage de points avec coloration par esp√®ce
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# S√©pales
scatter1 = axes[0].scatter(df['sepal length (cm)'], df['sepal width (cm)'],
                          c=df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}),
                          cmap='viridis', alpha=0.7, edgecolor='black')
axes[0].set_title('S√©pales : Longueur vs Largeur')
axes[0].set_xlabel('Longueur du s√©pale (cm)')
axes[0].set_ylabel('Largeur du s√©pale (cm)')

# P√©tales
scatter2 = axes[1].scatter(df['petal length (cm)'], df['petal width (cm)'],
                          c=df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2}),
                          cmap='viridis', alpha=0.7, edgecolor='black')
axes[1].set_title('P√©tales : Longueur vs Largeur')
axes[1].set_xlabel('Longueur du p√©tale (cm)')
axes[1].set_ylabel('Largeur du p√©tale (cm)')

plt.colorbar(scatter2, ax=axes, label='Esp√®ces (0=setosa, 1=versicolor, 2=virginica)')
plt.tight_layout()
plt.show()


# In[ ]:


# Boxplots pour chaque variable par esp√®ce
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.flatten()

for i, var in enumerate(variables):
    df.boxplot(column=var, by='species', ax=axes[i])
    axes[i].set_title(f'Distribution de {var} par esp√®ce')
    axes[i].set_xlabel('Esp√®ce')
    axes[i].set_ylabel('Valeur (cm)')
    axes[i].tick_params(axis='x', rotation=45)

plt.suptitle('')  # Supprime le titre automatique
plt.tight_layout()
plt.show()


# In[ ]:


# Calcul de la matrice de corr√©lation
corr_matrix = df.iloc[:, :-1].corr()

print("=== MATRICE DE CORR√âLATION ===")
display(corr_matrix)

# Visualisation
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Matrice de corr√©lation des variables')
plt.show()


# In[ ]:


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# S√©paration des donn√©es
X = df.drop('species', axis=1)
y = df['species']

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42, stratify=y)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("=== PR√âPARATION DES DONN√âES ===")
print(f"Taille de l'ensemble d'entra√Ænement : {X_train.shape}")
print(f"Taille de l'ensemble de test : {X_test.shape}")
print(f"Proportions dans y_train :")
print(y_train.value_counts(normalize=True))


# In[ ]:


from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Cr√©ation et entra√Ænement du mod√®le
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)

# Pr√©dictions
y_pred = knn.predict(X_test_scaled)

# √âvaluation
accuracy = accuracy_score(y_test, y_pred)

print("=== MOD√àLE KNN ===")
print(f"Exactitude : {accuracy:.2%}")
print(f"\nRapport de classification :")
print(classification_report(y_test, y_pred))

# Matrice de confusion
plt.figure(figsize=(6, 4))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Matrice de confusion - KNN')
plt.xlabel('Pr√©dictions')
plt.ylabel('Vraies valeurs')
plt.show()


# In[ ]:


from sklearn.model_selection import GridSearchCV

# D√©finition de la grille de param√®tres
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Recherche des meilleurs param√®tres
knn_base = KNeighborsClassifier()
grid_search = GridSearchCV(knn_base, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

print("=== OPTIMISATION KNN ===")
print(f"Meilleurs param√®tres : {grid_search.best_params_}")
print(f"Meilleur score (validation crois√©e) : {grid_search.best_score_:.2%}")

# √âvaluation du mod√®le optimis√©
best_knn = grid_search.best_estimator_
y_pred_best = best_knn.predict(X_test_scaled)
accuracy_best = accuracy_score(y_test, y_pred_best)

print(f"\nExactitude sur l'ensemble de test : {accuracy_best:.2%}")


# In[ ]:


from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# Liste des mod√®les √† comparer
models = {
    'KNN': KNeighborsClassifier(n_neighbors=3),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Naive Bayes': GaussianNB(),
    'SVM': SVC(random_state=42)
}

# Comparaison
results = []
for name, model in models.items():
    # Entra√Ænement
    model.fit(X_train_scaled, y_train)

    # Pr√©diction
    y_pred_model = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred_model)

    # Validation crois√©e
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')

    results.append({
        'Mod√®le': name,
        'Exactitude': accuracy,
        'CV Moyenne': cv_scores.mean(),
        'CV √âcart-type': cv_scores.std()
    })

# Affichage des r√©sultats
results_df = pd.DataFrame(results).sort_values('Exactitude', ascending=False)
print("=== COMPARAISON DES MOD√àLES ===")
display(results_df)

# Visualisation
fig, ax = plt.subplots(figsize=(10, 5))
x_pos = np.arange(len(results_df))
ax.bar(x_pos, results_df['Exactitude'], color='skyblue', edgecolor='black')
ax.set_xticks(x_pos)
ax.set_xticklabels(results_df['Mod√®le'], rotation=45)
ax.set_ylabel('Exactitude')
ax.set_title('Performance des diff√©rents mod√®les')
ax.set_ylim([0.8, 1.05])

# Ajouter les valeurs sur les barres
for i, v in enumerate(results_df['Exactitude']):
    ax.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()


# In[ ]:


# Fonction pour faire des pr√©dictions manuelles
def predict_iris(sepal_length, sepal_width, petal_length, petal_width):
    """Pr√©dit l'esp√®ce d'iris bas√©e sur les mesures fournies."""
    # Cr√©ation d'un DataFrame avec les mesures
    input_data = pd.DataFrame({
        'sepal length (cm)': [sepal_length],
        'sepal width (cm)': [sepal_width],
        'petal length (cm)': [petal_length],
        'petal width (cm)': [petal_width]
    })

    # Normalisation
    input_scaled = scaler.transform(input_data)

    # Pr√©diction avec le meilleur mod√®le
    prediction = best_knn.predict(input_scaled)[0]
    probabilities = best_knn.predict_proba(input_scaled)[0]

    # Affichage des r√©sultats
    print(f"\nüå∫ PR√âDICTION POUR VOS MESURES :")
    print(f"   Sepale : {sepal_length} cm x {sepal_width} cm")
    print(f"   P√©tale : {petal_length} cm x {petal_width} cm")
    print(f"\nüîÆ Esp√®ce pr√©dite : {prediction.upper()}")

    print("\nüìä Probabilit√©s :")
    for species, prob in zip(best_knn.classes_, probabilities):
        print(f"   {species}: {prob:.1%}")

    return prediction

# Exemple de pr√©diction
print("=== EXEMPLE DE PR√âDICTION ===")
predict_iris(5.1, 3.5, 1.4, 0.2)


# In[ ]:


# Pairplot (n√©cessite seaborn)
print("=== PAIRPLOT COMPLET ===")
sns.pairplot(df, hue='species', palette='husl', height=2.5)
plt.suptitle('Relations entre toutes les variables', y=1.02)
plt.show()

# Heatmap des corr√©lations avec annotations
plt.figure(figsize=(10, 8))
corr = df.iloc[:, :-1].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Matrice de corr√©lation (triangle sup√©rieur)')
plt.show()


# In[ ]:


import pickle

# Sauvegarde du mod√®le
with open('iris_knn_model.pkl', 'wb') as f:
    pickle.dump(best_knn, f)

# Sauvegarde du scaler
with open('iris_scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("=== CONCLUSION DU TP ===")
print("‚úì Mod√®le KNN sauvegard√© : iris_knn_model.pkl")
print("‚úì Scaler sauvegard√© : iris_scaler.pkl")
print("\nüìä R√âSUM√â DES R√âSULTATS :")
print(f"- Meilleur mod√®le : {type(best_knn).__name__}")
print(f"- Exactitude : {accuracy_best:.2%}")
print(f"- Param√®tres optimaux : {grid_search.best_params_}")

print("\nüéØ ACQUIS DU TP :")
print("1. Exploration des donn√©es ‚úì")
print("2. Visualisation avec matplotlib/seaborn ‚úì")
print("3. Pr√©paration des donn√©es ‚úì")
print("4. Entra√Ænement de mod√®les ‚úì")
print("5. √âvaluation et optimisation ‚úì")
print("6. Sauvegarde du mod√®le ‚úì")


# In[ ]:


import pickle
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# Charger les donn√©es
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target
df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

# Pr√©paration des donn√©es
X = df.drop('species', axis=1)
y = df['species']

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Entra√Ænement du mod√®le
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)

# Sauvegarder le mod√®le
with open('iris_model.pkl', 'wb') as f:
    pickle.dump(knn, f)

# Sauvegarder le scaler
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("‚úÖ Mod√®le et scaler sauvegard√©s !")
print("Fichiers cr√©√©s : iris_model.pkl et scaler.pkl")


# In[ ]:




